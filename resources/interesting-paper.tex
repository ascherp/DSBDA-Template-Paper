\section{Resources: Very Interesting Paper}

\paragraph{Language Model ``Alternatives''}

\begin{itemize}
\item S4: Efficiently Modeling Long Sequences with Structured State Spaces, \url{https://arxiv.org/abs/2111.00396}

\item Mamba: Linear-Time Sequence Modeling with Selective State Spaces, \url{https://arxiv.org/abs/2312.00752}

\item xLSTM: Extended Long Short-Term Memory, \url{https://arxiv.org/abs/2405.04517}

\item KAN: Kolmogorov-Arnold Networks, \url{https://arxiv.org/abs/2404.19756}

\item gMLP: Pay Attention to MLPs, \url{https://arxiv.org/abs/2105.08050}

\item Pretraining Without Attention, \url{https://arxiv.org/abs/2212.10544}
\end{itemize}

\paragraph{Language}

\begin{itemize}
\item Do Llamas Work in English? On the Latent Language of Multilingual Transformers, \url{https://arxiv.org/abs/2402.10588}

\item Evolutionary Optimization of Model Merging Recipes, 
\url{https://arxiv.org/abs/2403.13187}

TL;DR: Language model merging on the level of layers.

\item When LLMs are Unfit Use FastFit: Fast and Effective Text Classification with Many Classes,
\url{https://arxiv.org/abs/2404.12365v1}

\item Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs,
\url{https://aclanthology.org/2024.eacl-long.5/}
\newline
and
\newline
Proving Test Set Contamination in Black-Box Language Models, \url{https://openreview.net/forum?id=KS8mIvetg2}

\item LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders
\url{https://arxiv.org/abs/2404.05961}

\end{itemize}

\paragraph{Graphs}

\begin{itemize}

\item GraphAny: A Foundation Model for Node Classification on Any Graph, \url{https://arxiv.org/abs/2405.20445}

\item Mirage: Model-Agnostic Graph Distillation for Graph Classification, \url{https://arxiv.org/abs/2310.09486}
\end{itemize}


\section{Resources: Interesting Paper}

\paragraph{Language}

\begin{itemize}

\item MM1: Methods, Analysis \& Insights from Multimodal LLM Pre-training,
\url{https://arxiv.org/abs/2403.09611} 
\newline
TL;DR: Show among others that adding automatically generated image captions improves the image classification task.

\item Better \& Faster Large Language Models via Multi-token Prediction, 
\url{https://arxiv.org/abs/2404.19737}

TL;DR: Learn to predict $n$ tokens in causal language modeling rather than a single token is better.

\end{itemize}

\paragraph{Graphs}

\begin{itemize}

\item Graph Language Models,
\url{https://arxiv.org/abs/2401.07105}

TL;DR: Transfer of T5's model weights and modification of the attention mechanism to support graphs.

\item Utilizing Description Logics for Global Explanations of Heterogeneous Graph Neural Networks, \url{https://arxiv.org/abs/2405.12654}

TL;DR: Bridges the classical world of logic to fancy graph neural networks.

\end{itemize}

